\relax 
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}I. Introduction}{1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}II. Theory}{1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}A. Random Forest}{1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}B. XG Boost}{1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The evolution of classification accuracy with changing parameters of the random forest, where $NT$ is the number of trees, $MT$ are the number of variables randomly sampled, and $NS$ is a depth control parameter.}}{2}{}}
\newlabel{vargraph}{{1}{2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}III. Numerical evaluation and optimisation}{2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}A. Algorithm parameters}{2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}B. Added features}{2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The classification accuracy for a random forest (RF) with and without (NF) generated features. The means are shown as dashed lines.}}{3}{}}
\newlabel{featgraph}{{2}{3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The classification accuracy for the XG boost (XGB) and the random forest (RF). The means are shown as dashed lines. Both have features added.}}{3}{}}
\newlabel{xgrf}{{3}{3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}C. Combination of models}{3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Table summarising the accuracy for different methods on the first three classes. The final two are not shown because all methods have similarly low accuracy, and due to low frequency of these classes, they do not significantly affect the overall accuracy. Note that there are significantly more popularity $2$ values than $1$ or $3 $.}}{4}{}}
\newlabel{cattable}{{I}{4}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The classification accuracy for the combined classifier (alKK) and the XG boost (XGB). The means are shown as dashed lines.}}{4}{}}
\newlabel{compgraph}{{4}{4}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The classification accuracy for the combined classifier when trained with and without the $4$ and $5$ popularity rows.}}{4}{}}
\newlabel{remg}{{5}{4}{}{}{}}
\bibstyle{apsrev}
\@writefile{toc}{\contentsline {section}{\numberline {}IV. Evaluation}{5}{}}
\newlabel{LastPage}{{}{5}}
